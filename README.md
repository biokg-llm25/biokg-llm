# Construction of Biomedical Knowledge Graphs Using Large Language Models
<div align="left">
  <a href="biokg_llm.csv" download>
    <img src="https://img.shields.io/badge/DOWNLOAD%20DATASET-CSV-blue?style=for-the-badge&logo=csv&logoColor=white" alt="Download Dataset CSV">
  </a>
</div>

## TL;DR

We introduce a two-fold contribution:

1️⃣ A **hackable, end-to-end pipeline** that constructs biomedical knowledge graphs from unstructured web data using a **web scraper, fuzzy matching, and large language models (LLMs)**.  
2️⃣ A **curated dataset** generated by applying this method to publicly available **drug leaflets** sourced from online pharmacies — capturing drug-centric attributes often missing in existing biomedical KGs.

**[Download the KG Dataset (CSV)](biokg_llm.csv)** to explore clinically relevant information beyond molecular data.

<p align="center">
  <img src="figures/Proposed_Pipeline.png" alt="Pipeline Overview" width="100%">
</p>
<em>Figure 1: Proposed Pipeline</em>


## Overview

Knowledge graphs (KGs) are increasingly used to represent biomedical information in structured, interpretable formats. However, existing biomedical KGs often focus narrowly on molecular interactions or adverse events, overlooking the rich, drug-centric data found in package leaflets.

In this work, we present: (1) An end-to-end pipeline for automatically constructing biomedical KGs from unstructured text using an LLM, and (2) A dataset in the form of a knowledge graph, created by applying the proposed method to publicly available drug package inserts sourced from online pharmacy websites.

The proposed method is modular, reproducible, and easy to adapt. Both the LLM and the input data can be substituted to generate different KGs. The resulting KG captures clinically relevant attributes such as side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions, and physical characteristics, which are often missing in existing medical databases and KGs.

We evaluate the KG through manual inspection and compare its coverage with existing biomedical KGs and databases, highlighting complementary aspects such as storage instructions, dosage guidelines, and ingredient details that are often absent in these resources. We expect this dataset to support future applications in areas such as patient safety and drug recommendation, while the generation method can be easily adopted for other types of unstructured documents.

## Features

- *End-to-End Biomedical KG Pipeline:*  A modular and hackable pipeline that extracts structured knowledge from unstructured text using web scraping, LLMs, NER, and relation mapping. Both the LLM and scraper components can be easily replaced or customized.

- *Drug Leaflet-Centric Knowledge Graph:*  Constructed from 13,000+ drug leaflets sourced from the HPRA, capturing real-world, patient-facing clinical data often overlooked in existing biomedical resources.

- *LLM-Based Information Extraction:*  Utilizes LLaMA 3 70B Instruct for prompt-based extraction of subject–predicate–object triples directly from full-text PDFs, avoiding the need for chunking and preserving context.

- *Ready-to-Use Dataset:*  The resulting knowledge graph can be downloaded [here](https://github.com/biokg-llm25/biokg-llm/blob/main/data/biomedical_kg.csv) in a ready-to-use CSV format.

- *Broader Clinical Coverage:*  Captures practical features often missing in other biomedical KGs and databases, including storage information, physical appearance (shape, color), and inactive ingredients.

- *Generalizable & Open Source:*  The pipeline is reusable and adaptable across domains and document types, and the full codebase is open source.

## Proposed Pipeline

We present an end-to-end pipeline for constructing a biomedical knowledge graph from unstructured drug leaflet data. The process begins with data collection, where drug leaflets are scraped from online pharmacies using a web scraper built using Python and converted into machine-readable text. This raw data is then processed using a prompt-based approach with an LLM for information extraction, focusing on key drug-related entities and their interrelationships. The extracted entities undergo further refinement through NER and relation mapping, ultimately being transformed into nodes and labeled edges. These are subsequently post-processed and organized into a CSV format, which serves as the foundation of the knowledge graph. [Figure 1](#figure-1-proposed-pipeline) above provides a visual overview of the entire pipeline, illustrating each processing stage from data collection to knowledge graph construction.

## Dataset Description

As a demonstration of our general pipeline for the construction of biomedical KGs, we implemented and evaluated a case study focused on drug leaflets. The resulting KG contains a total of 488,154 subject–predicate–object triples, covering 23,313 unique entities and 10 different relation types. It comprises 4,786 unique Drug entities, connected to a wide range of medically relevant object types. Specifically, the graph includes 8,043 SideEffect nodes, 7,213 Warning nodes, 4,016 Contraindication nodes, 1,950 ActiveIngredient nodes, and 1,781 InactiveIngredient nodes. Additional descriptive attributes are captured through 926 Colour, 1,143 Shape, 660 StorageInfo, and 177 DosageInfo nodes. The graph forms a single connected component, with all 23,313 nodes interconnected via 431,909 directed, labeled edges. The average node degree is 41.87, indicating a densely connected structure. The following figures 2 and 3 show the distribution of various entity types and relation types in the dataset.

<table width="100%">
  <tr>
    <td align="center" width="50%">
      <img src="figures/Entity_Distribution.png" alt="Figure 1" width="100%"/><br>
      <em>Figure 2: Entity Type Distribution</em>
    </td>
    <td align="center" width="50%">
      <img src="figures/Edge_Distribution.png" alt="Figure 2" width="100%"/><br>
      <em>Figure 3: Relation Type Distribution</em>
    </td>
  </tr>
</table>



## Installation

Follow these steps to set up the project locally:

### 1. Clone the repository
```bash
git clone https://github.com/biokg-llm25/biokg-llm.git
cd biokg-llm
```

### 2. Create and activate virtual environment
```bash
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

## Usage
The pipeline is divided into multiple stages for modular processing:

### 1. Data Scraping
- **Step 1: Extract Source Code**
```bash
python scripts/scrape_html_sources.py
```
Scrapes the [HPRA website](https://www.hpra.ie) using BeautifulSoup and extracts source code with links to drug leaflet pages. You may use a scraper of your choice.
- **Step 2: Download PDFs from Extracted URLs**
```bash
python scripts/download_pdfs.py --rtf_path ./sourcecode_hpra.rtf --output_folder ./hpra_data
```
Downloads the actual drug leaflet PDFs based on the extracted page source code.

### 2. Information Extraction Using LLM
- **Step 3: Extract Structured Data from PDFs**
```bash
python scripts/extract_information.py \
  --pdf_dir ./hpra_data \
  --output ./hpra_kg.txt \
  --llm_url <your_llm_url> \
  --llm_model <your_model_name>
```
Uses a locally hosted LLM (e.g., LLaMA 3 70B Instruct) to extract drug-related information using prompts. Outputs are saved as Question(Prompt)-Answer(LLM Response) pairs in a single .txt file. Replace the LLM arguments with your preferred model and endpoint.


### 3. Knowledge Graph Construction
- **Step 4: Extract Normalized Medical Terms (Preprocessing)**
```bash
python scripts/extract_medical_terms.py --input ./hpra_kg.txt --output ./final_medical_terms_hpra.txt
```
Generates a list of medical terms from the Q&A-format .txt file using named entity recognition (NER). These terms are used for fuzzy matching and normalization in the subsequent KG construction step.
- **Step 5: Build Initial KG in CSV Format**
```bash
python scripts/build_kg_csv.py --input hpra_kg.txt --terms final_medical_terms_hpra.txt --output hpra_complete_network.csv
```
Parses the extracted .txt file and and the curated medical terms list, performs entity recognition and relation mapping, and constructs a raw KG in CSV format.
- **Step 6: Post-Processing & Final KG Generation**
```bash
python scripts/postprocess_kg.py \
  --input ./hpra_complete_network.csv \
  --final_output ./hpra_final_network.csv \
  --log_dir ./logs \
  --llm_url <your_llm_url> \
  --llm_model <your_model_name>
```
Performs post-processing and cleaning like entity shortening using a locally hosted LLM (LLaMA 3 70B Instruct). Replace the LLM arguments with your preferred model and endpoint.

### 4. Visualization and KG Statistics
```bash
jupyter notebook graph_stats.ipynb
```
Explore key graph statistics of the dataset(as a kG) like:
- Entity & relation type distributions
- Node & edge counts
- Degree distributions and graph connectivity
